
Import des librairies et de méthodes :


```python
seed = 7
%matplotlib inline

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import cross_validation, grid_search

from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import VotingClassifier
```

# 1. Import des donnees


```python
# Spécification des colonnes
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']

# Import
data = pd.read_table('data.txt', names=names, sep=',')

# Spécification des variables (X) et de la cible (Y)
X = data.iloc[:, 0:8]
Y = data.iloc[:, 8]
```

Regardons la taille de notre jeu de données et quelques exemples :


```python
print(X.shape)

print(X.iloc[:10])
```

    (768, 8)
       preg  plas  pres  skin  test  mass   pedi  age
    0     6   148    72    35     0  33.6  0.627   50
    1     1    85    66    29     0  26.6  0.351   31
    2     8   183    64     0     0  23.3  0.672   32
    3     1    89    66    23    94  28.1  0.167   21
    4     0   137    40    35   168  43.1  2.288   33
    5     5   116    74     0     0  25.6  0.201   30
    6     3    78    50    32    88  31.0  0.248   26
    7    10   115     0     0     0  35.3  0.134   29
    8     2   197    70    45   543  30.5  0.158   53
    9     8   125    96     0     0   0.0  0.232   54
    

# 2 Construction de différents modèles

Nous voulons prédire si une femme âgée de la tribu indienne Pima, de plus de 21 ans, habitant à Phoenix, présente ou non des signes de diabète, selon l'âge, le nombre de grossesses, la concentration en glucose, la pression systolique, etc.

## 2.1 Estimation réaliste de la performance des modèles par validation croisée


```python
kfold = cross_validation.KFold(n=len(X), n_folds=20)
```

## 2.2 Modèles

### 2.2.1 Méthodes basées sur les observations proches

**2.2.1.1** k plus proches voisins


```python
knn = KNeighborsClassifier(n_neighbors=10)

results_knn = list(range(10))

for i in range(10):
    results = cross_validation.cross_val_score(model, X, Y, cv=kfold)
    results_knn[i] = results.mean()
    
print(sum(results_knn)/float(len(results_knn)))
```

    0.774291497976
    

### 2.2.2 Méthodes d'ensemble

**2.2.2.1 Bagging**

Le bagging revient à calculer différents modèles sur des échantillons (tirés avec remise) du jeu de données entier.

**Bagging d'arbres de décision**

Agrégation par vote à main levée d'un certain nombre d'arbres de décisions entrainés sur des échantillons bootstrap de la population totale


```python
cart = DecisionTreeClassifier()
model = BaggingClassifier(base_estimator=cart, n_estimators=100)

results_bag_cart = list(range(10))

for i in range(10):
    results = cross_validation.cross_val_score(model, X, Y, cv=kfold)
    results_bag_cart[i] = results.mean()

print(sum(results_bag_cart)/float(len(results_bag_cart)))
```

    0.765010121457
    

**RandomForest**

Bagging d'arbres de décision modifié : pour chaque arbre, il faut décider de manière itérative la meilleure coupure à opérer. Ici, chaque coupure n'est plus choisie comme étant la meilleure coupure parmi toutes les variables, mais la meilleure coupure pour un sous-ensemble des variables explicatives.


```python
model = RandomForestClassifier(n_estimators=100, max_features='auto')

results_rf = list(range(10))

for i in range(10):
    results = cross_validation.cross_val_score(model, X, Y, cv=kfold)
    results_rf[i] = results.mean()
    
print(sum(results_rf)/float(len(results_rf)))
```

    0.764014844804
    

**ExtraTrees**

RandomForest modifiées : pour chaque coupure, la coupure n'est pas choisie comme étant la meilleure coupure des variables sélectionnées, mais la meilleure parmi un sous-ensemble de toutes les coupures.


```python
model = ExtraTreesClassifier(n_estimators=100, max_features='auto')

results_et = list(range(10))

for i in range(10):
    results = cross_validation.cross_val_score(model, X, Y, cv=kfold)
    results_et[i] = results.mean()
    
print(sum(results_et)/float(len(results_et)))
```

    0.757995951417
    

**2.2.2.2 Boosting**

Le boosting consiste à créer itérativement un certain nombre de modèles, chaque modèle étant censé réduire les erreurs de prédiction du modèle précédent.
Le modèle final est une combinaison linéaire de tous les modèles, pondérée par la précision de chaque modèle.

**Adaboost**

_TODO_


```python
model = AdaBoostClassifier(n_estimators=40)

results_ada = list(range(10))

for i in range(10):
    results = cross_validation.cross_val_score(model, X, Y, cv=kfold)
    results_ada[i] = results.mean()
    
print(sum(results_ada)/float(len(results_ada)))
```

    0.763495276653
    

**Gradient Boosting**

_TODO_


```python
model = GradientBoostingClassifier(n_estimators=100)

results_gb = list(range(10))

for i in range(10):
    results = cross_validation.cross_val_score(model, X, Y, cv=kfold)
    results_gb[i] = results.mean()
    
print(sum(results_gb)/float(len(results_gb)))
```

    0.77298245614
    

**2.2.2.3 Ensembles de différents modèles**

**Agrégation par vote**

C'est la manière la plus simple d'agréger les prédictions de différents modèles. Il s'agit simplement de décider de prédire la classe dominante parmi les modèles


```python
results_vote = list(range(10))

for i in range(10):
    m1 = DecisionTreeClassifier()
    m2 = GaussianNB()
    m3 = LogisticRegression()
    m4 = SVC()
    
    ensemble = VotingClassifier(estimators=[('tree', m1), ('nb', m2), ('lr', m3), ('svm', m4)])
    results = cross_validation.cross_val_score(ensemble, X, Y, cv=kfold)
    results_vote[i] = results.mean()
    
print(sum(results_vote)/float(len(results_vote)))
```

    0.735003373819
    

# 3 Amélioration des scores en réglant les hyperparamètres

## 3.1 Réglage d'1 hyperparamètre par _grid search_ et validation croisée


```python
model = SVC(kernel='linear')
# parameters = {'kernel':('linear', 'rbf'), 'C':[0.05, 0.1, 0.5, 1, 5]}
parameters = {'C':[0.05, 0.1, 0.5, 1, 5]}

clf = grid_search.GridSearchCV(model, param_grid=parameters, cv=kfold, n_jobs=-1)
clf.fit(X, Y)

# Graphique
# plt.figure(1, figsize=(4, 3))
# plt.clf()
# plt.semilogx(valeurs_c, scores)
# plt.ylabel('Précision')
# plt.xlabel('Paramètre C')
# plt.show()
```




    GridSearchCV(cv=sklearn.cross_validation.KFold(n=768, n_folds=20, shuffle=False, random_state=None),
           error_score='raise',
           estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False),
           fit_params={}, iid=True, n_jobs=1,
           param_grid={'C': [0.05, 0.1, 0.5, 1, 5]}, pre_dispatch='2*n_jobs',
           refit=True, scoring=None, verbose=0)




```python
clf.grid_scores_
```




    [mean: 0.76693, std: 0.06900, params: {'C': 0.05},
     mean: 0.77083, std: 0.06534, params: {'C': 0.1},
     mean: 0.76823, std: 0.07211, params: {'C': 0.5},
     mean: 0.77214, std: 0.07618, params: {'C': 1},
     mean: 0.76823, std: 0.07107, params: {'C': 5}]




```python

```
